{"pages":[],"posts":[{"title":"ConcurrentHashMap 1.7与1.8区别","text":"1、整体结构1.7：Segment + HashEntry + Unsafe（数组+链表） 1.8: 移除Segment，使锁的粒度更小，Synchronized + CAS + Node + Unsafe （数组+链表+红黑树） 2、put（）1.7：先定位Segment，再定位桶，put全程加锁，没有获取锁的线程提前找桶的位置，并最多自旋64次获取锁，超过则挂起。 1.8：由于移除了Segment，类似HashMap，可以直接定位到桶，拿到first节点后进行判断， 为空则CAS插入； 为-1则说明在扩容，则跟着一起扩容； else则加锁put（类似1.7） 3、get（）基本类似，由于value声明为volatile，保证了修改的可见性，因此不需要加锁。 4、resize（）1.7：跟HashMap步骤一样，只不过是搬到单线程中执行，避免了HashMap在1.7中扩容时死循环的问题，保证线程安全。 1.8：支持并发扩容，HashMap扩容在1.8中由头插改为尾插（为了避免死循环问题），ConcurrentHashmap也是，迁移也是从尾部开始，扩容前在桶的头部放置一个hash值为-1的节点，这样别的线程访问时就能判断是否该桶已经被其他线程处理过了。 5、size（）1.7：很经典的思路：计算两次，如果不变则返回计算结果，若不一致，则锁住所有的Segment求和。 1.8：用baseCount来存储当前的节点个数，这就设计到baseCount并发环境下修改的问题。","link":"/2020/04/20/ConcurrentHashMap-1-7%E4%B8%8E1-8%E5%8C%BA%E5%88%AB/"},{"title":"gnuradio_ubuntu16","text":"GNU Radio 1.更新软件包列表，更新系统软件 sudo apt-get update sudo apt-get upgrade 2. 安装依赖包(仅适用于UBUNTU 16.04，其他版本需要的依赖包会有所不同)sudo apt-get install libfontconfig1-dev libxrender-dev libpulse-dev swig g++ automake autoconf libtool python-dev libfftw3-dev libcppunit-dev libboost-all-dev libusb-dev libusb-1.0-0-dev fort77 libsdl1.2-dev git-core libqt4-dev python-numpy ccache python-opengl libgsl-dev python-cheetah python-mako python-lxml doxygen qt4-default qt4-dev-tools libusb-1.0-0-dev libqwt5-qt4-dev libqwtplot3d-qt4-dev pyqt4-dev-tools python-qwt5-qt4 cmake git-core wget libxi-dev python-docutils gtk2-engines-pixbuf r-base-dev python-tk liborc-0.4-0 liborc-0.4-dev libasound2-dev python-gtk2 libzmq-dev python-requests python-sphinx python-zmq libncurses5 libncurses5-dev python-wxgtk3.0 python-scipy python-matplotlib 3. 下载UHD 源文件并编译安装（选择你认为合适的文件目录下）==PS：一定要先安装uhd不然会没有usrp模块== git clone git://github.com/EttusResearch/uhd.git（若提示git 软件未安装，则按照提示输入sudo apt-get install git 安装） 下载的源文件有不同release 版本，通过如下操作找到最新的release 版本或者你需要的版本：cd uhd然后在终端输入git checkout release 连续按tab 键，将会打印出可选的release 版本，然后输入git checkout release_003_011_000_000（切换到合适版本，这里选择3.11.0） cd host mkdir build cd build cmake ../ make 使用make -j4并行编译 make test sudo make install sudo ldconfig 4. 下载UHD 镜像文件and烧写镜像文件。==要是usrp已经烧写过镜像可省略==sudo uhd_images_downloader 如果你安照步骤操作，编译，安装不会报错。此时UHD驱动已经安装完毕，可通过输入uhd_find_devices 可看到USRP设备的信息（确保电脑与设备处于同一网络下。注意usrp n210必须使用千兆网线与pc段通信，确保pc机有千兆网卡，且网线为千兆网线。当时在这里走了弯路。具体设置查看http://www.ettus.com.cn/peixun/28/）uhd_image_loader –args=”type=usrp2,addr=192.168.10.2（默认的ip地址）,reset” 5. 下载GNU Radio 源文件并编译安装（步骤类似,因为都是下载源文件,安装啊，选择合适文件夹） git clone –recursive git://github.com/gnuradio/gnuradio.git cd gnuradio/ 切换到release 版本，可以通过以下方式查找最新的release 版本：git checkout v3 连续按tab 键，将会打印出可选的release 版本，找到版本号最高的版本，比如v3.7.9.1。 切换到该最高版本：git checkout v3.7.9.1 ==PS：一定要切换到最新版本== mkdir build cd build cmake ../ make(有些会编译失败，不影响使用）==make -j4 使用4个线程编译 速度会提升大约20分钟即可编译完成== make test sudo make install sudo ldconfig 如果一切顺利，到此就已经ok了。 终端下输入 gnuradio-companion就能打开软件了，此时你自己写一个小程序或者打开一个example.grc。网上关于”hello，world“似的例子有很多，百度即可。","link":"/2019/04/09/gnuradio-install/"},{"title":"卡尔曼与粒子滤波","text":"现有⼀辆在路上做直线运动的小车。假设小车k时刻的速度为$u_{k}$，设k-1时刻小车的位置为$x_{k-1}$，则k时刻小⻋的位置$x_{k}$ 为：$$x_{k}=x_{k-1}+\\nabla t * u_{k}+w_{k}$$ 其中$w_{k}$ 为过程噪声，$w_{k} \\sim N(0, Q)$， GPS在k时刻对小车行驶距离的观测值为$y_{k}$，GPS存在测量噪声$v_{k} \\sim N(0, R)$ $$y_{k}=x_{k}+v_{k}$$ 现⽤卡尔曼滤波算法对小车的行驶距离进⾏估计： 写出k时刻小车行驶距离估计值的更新迭代方程 ： $$ \\hat x_{k}^{-}=F_{k} {x}_{k-1} \\tag{1} $$ $$ \\Sigma_{k}^{-}=F \\Sigma_{k-1} F^{T}+Q \\tag{2} $$ $$ K_{k}=\\Sigma_{k}^{-} H^{T}\\left(H \\Sigma_{k}^{-} H^{T}+R\\right)^{-1} \\tag{3} $$ $$ \\hat x_{k} = \\hat x_{k}^{-}+K_{t} \\left (y_{k}-H \\hat x_{k}^{-} \\right) \\tag{4} $$ $$ \\Sigma_{k}=\\left(I-K_{k} H\\right) \\Sigma_{k}^{-} \\tag{5} $$ 其中（1）（2）为预测方程，（3）（4）（5）为更新迭代方程。$\\hat x_{k}^{-}=\\begin{bmatrix} p_{k} \\\\ v_{k} \\\\\\end {bmatrix} =\\begin {bmatrix} 1 &amp; \\Delta t \\\\ 0 &amp; 1\\\\\\end {bmatrix}\\begin {bmatrix}p_{k-1} \\\\v_{k-1} \\\\\\end {bmatrix}$，$\\quad F_{k}=\\begin {bmatrix} 1 &amp; \\Delta t \\\\ 0 &amp; 1 \\\\\\end {bmatrix}$，其中 $F_{k}$ 为状态转移矩阵，$\\boldsymbol{H}=\\left[ \\begin{array}{ll}{1} &amp; {0}\\end{array}\\right]$ 。 $\\Sigma_{k}$ 为协方差矩阵，表示每一时刻的不确定性。 $K$ 为卡尔曼系数，为一个矩阵，是对残差的加权矩阵，也称滤波增益矩阵。 $Q$ ,$R$ 分别为预测噪声与测量噪声。 如果测量噪⾳很小，考虑极端情况，测量噪声的⽅差$R$趋向于0，对小车行驶距离的估计受测量值影响较大还是估计值影响较z？写出推导过程。 $lim_{R \\rightarrow 0} K_{k}=H^{-1}$，观测噪声协方差$R$ 越小，残余的增益越大 K 越大。$y_{k}$ 的权重变大，即受测量值影响大。 相反，当$Q$ 趋向于0，有：$lim_{Q \\rightarrow 0} K_{k}=0$ ，$\\hat x_{k}^{-}$ 的权重变大，即受估计值影响大。 PS: 公式（3）的推导见博客卡尔曼滤波 – 从推导到应用(一) 粒子滤波习题 有⼀可以在⼆维平⾯上移动的小车（图中灰⾊圆点）。每隔固定的⼀段时间⽤粒子滤波算法定位小车的位置。有四个粒⼦x1，x2，x3，x4，x1[(1,5),0.25]，x2[(3,5),0.25]，x3[(3,7),0.25]，x4[(5,5),0.25]（x[(横坐标x，纵坐标y)，权重w]，图中红⾊星星）。 假设我们现在知道了车的运动向量为(5,4)（图中蓝色箭头）。求小车位置的预测值。 (1,5) * 0.25 + (3,5) * 0.25 + (3,7) * 0.25 + (5,5) * 0.25 + (5,4) = (8,9.5) 此时小车上GPS对小车位置的定位为(8,10)，GPS的测量误差服从⼆维标准正态分布。请按照GPS对定位的测量值对四个粒⼦重新分配权重。 12345678910111213141516171819202122232425262728293031import numpy as npimport scipy.statsfrom numpy.random import uniform, randn, randomdef update(particles, weights, z, R, landmarks): weights.fill(1.) for i, landmark in enumerate(landmarks): distance = np.linalg.norm(particles[:, 0:2] - landmark, axis=1) weights *= scipy.stats.norm(distance, R).pdf(z[i]) weights += 1.e-300 # avoid round-off to zero weights /= sum(weights) # normalizeif __name__ == '__main__': vector = np.array([[5,4]]) #移动向量 particles = np.array([[1,5],[3,5],[3,7],[5,5] ]) particles += vector #粒子位置 weights = np.array([0.25,0.25,0.25,0.25]) #初始权重 pos =np.array( [[8,9.5]]) #预测位置 landmark = np.array([[8,10]]) #GPS定位（测量位置） NL = len(landmark) sensor_std_err = 1 #测量误差（二维标准正态分布） z = np.linalg.norm(pos-landmark,axis=1) #z = dist(预测位置，测量位置) update(particles,weights,z,sensor_std_err,landmark) print(weights) 重新分配的权重为：$[0.10034672,0.39965328,0.39965328,0.10034672]$ 粒子更新为：$[[ 6 , 9] [ 8 , 9] [ 8 ,11] [10 , 9]]$ 在前两问的基础上，写出重采样算法估计小车的位置的伪代码. 采用多项式重采样，基本步骤如下： $$ F(x)=P(X \\leq x)=\\sum_{x^{i}&lt;x} p\\left(x_{i}\\right) $$ 将 $[0,1]$ 区间分为 $N$ 个子区间 $\\left(0, p\\left(x^{1}\\right)\\right]$ ，$\\left(p\\left(x^{1}\\right), p\\left(x^{1}\\right)+\\right.$$p\\left(x^{2}\\right) ]$，···，$\\left(\\sum_{m=1}^{N-1} p\\left(x^{m}\\right), \\sum_{m=1}^{N} p\\left(x^{m}\\right)\\right]$。设$U$是 $[0,1]$ 区间上的均匀分布时间变量，根据$U$的值落到何区间，相应的区间对应的随机变量就是所需输出量，假设落在第$j$区间，则输出为$x^{j}$，粒子的子代数 $n^{i}$ 表示 $U$ 的值落在该区间的次数。 伪代码如下： 产生[0,1]上均匀分布随机数 $\\left\\{u_{i}\\right\\}_{j=1, n}$ ; 产生粒子权重累积函数$wc$，满足$w c(i)=\\sum_{m=1}^{i} w_{k}^{m}$ ; k=1; for i=1:N while (wc(k)&lt;u(i)) k = k+1; end index(i) = k; //表示第k个粒子经重采样后被复制在第i个位置 end 运行代码如下： 12345678910def simple_resample(particles, weights): N = len(particles) cumulative_sum = np.cumsum(weights) cumulative_sum[-1] = 1. # avoid round-off error indexes = np.searchsorted(cumulative_sum, random(N)) # resample according to indexes particles[:] = particles[indexes] weights[:] = weights[indexes] weights /= np.sum(weights) # normalize PS：部分代码转自机器人粒子滤波定位（蒙特卡罗定位）","link":"/2019/05/02/%E5%8D%A1%E5%B0%94%E6%9B%BC%E4%B8%8E%E7%B2%92%E5%AD%90%E6%BB%A4%E6%B3%A2/"},{"title":"python_with_matlab","text":"前言 近期chirp—ook背向通信工作遇到了瓶颈。tag的硬件上FPGA反射OOK不够精确其他原因，通信工作暂停。现转为无线感知，具体为基于chirp信号的backscatter的动作识别。 考虑动作识别，那么ML必然逃不过。根据先前的RFID定位的经验，任何复杂情景下的感知，ML都较于传统模型来的快。老板一直说不要吃快餐，可是别说是快餐，在不学习ML可能连毕业都是问题。 由于处理信号数据matlab有天然的优势，而deeplearning又逃不过python。于是python与matlab的交换使用不失为一种解决方案！ 1. 安装 Python3.6 + anaconda3 + Matlab2018a-转自Python 的 MATLAB 引擎 API- 要在 Python® 会话内启动 MATLAB® 引擎，必须先安装 Python 包形式的引擎 API。MATLAB 提供了标准的 Python setup.py 文件，用于通过 distutils 模块编译和安装引擎。您可以使用相同的 setup.py 命令在 Windows®、Mac 或 Linux® 系统上编译和安装引擎。 在安装之前，确认您的 Python 和 MATLAB 配置。 检查您的系统是否具有受支持的 Python 版本和 MATLAB R2014b 或更新版本。要检查您的系统上是否已安装 Python，请在操作系统提示符下运行 Python。 将包含 Python 解释器的文件夹添加到您的路径（如果尚未在该路径中）。 找到 MATLAB 文件夹的路径。启动 MATLAB，并在命令行窗口中键入 matlabroot。复制 matlabroot 所返回的路径。 要安装引擎 API，请选择以下选项之一。 在 Windows 操作系统提示符下 - 12cd &quot;matlabroot\\extern\\engines\\python&quot;python setup.py install 您可能需要管理员权限才能执行这些命令。 在 macOS 或 Linux 操作系统提示符下 - 12cd &quot;matlabroot/extern/engines/python&quot;python setup.py install 2. 使用 如果你要自定义一个函数(function.m)，需将该文件放在于python工程下。 下例为使用matlab提供的python API，读一个二进制复数文件(信号的IQ信息)。 123456789101112import matplotlib.pyplot as pltimport matlab.enginedef data_input(filename): eng = matlab.engine.start_matlab() data = eng.read_complex_binary(filename,1e7) plt.plot(eng.abs(data[1:10**5])) plt.show()if __name__ == '__main__': data_input('./data/0423/gesture1.dat') 123456789101112131415161718192021222324252627function v = read_complex_binary (filename, count) %% usage: read_complex_binary (filename, [count]) %% %% open filename and return the contents as a column vector, %% treating them as 32 bit complex numbers %% m = nargchk (1,2,nargin); if (m) usage (m); end if (nargin &lt; 2) count = Inf; end f = fopen (filename, 'rb'); if (f &lt; 0) v = 0; else t = fread (f, [2, count], 'float'); fclose (f); v = t(1,:) + t(2,:)*i; [r, c] = size (v); v = reshape (v, c, r); end 信号的时域图 3. 注意！ 网上有很多教程为mlab的安装教程，但mlab十分不适用于python3+。官方的文档给的版本为python2.7。尝试了很久，安装都不成功。个人不介意使用mlab。","link":"/2019/04/23/python-with-matlab/"},{"title":"机器学习:决策树","text":"本文尽量采用最简洁最直白的描述，对于有些解释有出入的恳请指出。 前言：经过两次算法的面试，发现工业界对于机器学习主要是在分布式和效率上，对于精度的取舍可能不是很看重。因此，集成学习和深度学习是要掌握透彻的。（XGboost、GBDT和RF几乎必问） 什么是决策树？决策树是一种采用属性(特征)划分来解决分类问题的算法，通常有3个步骤： 特征选择：对于多维特征，依次计算不同维度上特征的信息增益。 决策树生成：选取最大信息增益的特征作为根节点，依次生成子节点。 剪枝：对抗过拟合，去除分支。（预剪枝，后剪枝） 3种决策树(划分)算法 ID3：采用信息增益，最早的决策树算法 C4.5：采用信息增益比 CART(Classification and regression Tree)：采用基尼系数 优缺点 优点 缺点 运行速度快，能够应对大型数据源 易过拟合 适合有缺失属性的样本 忽略特征之间的关联 可以同时处理标称型与数值型数据 类别样本不均衡时，ID3偏好数目多的属性，CART偏好数目少的属性 可视化 连续和缺失值 连续值：类型属性离散化，如二分法 缺失值： 如何属性缺失情况下划分？（给定样本权重，权重等价于属性缺失占的比例） 给定划分后属性后，样本的属性缺失如何划分？（让同一个样本以不同概率分到不同子节点） 机器学习—RF、GBDT、XGBoost前言：RF、GBDT、XGBoost都是集成学习，即组合多个基学习器预测结果来得到最终的结果，提高了模型的泛化性和鲁棒性。分为两类： 基学习器之间强依赖，必须串行生成：Boosting 基学习器之前弱依赖，可并行生成：Bagging、RF Bagging与随机森林原理：相互交替的采用子集训练不同的学习器。主要关注降低方差，在不剪枝决策树、神经网络等易受样本扰动的学习器上效用更为明显。 Bagging： 放回抽样 多数表决（分类）或简单平均（回归） RF： 随机选择样本（放回抽样） 随机选择特征 构建决策树 随机森林投票（平均）。 Boosting原理：不断迭代，调整分布。主要关注降低偏差，提高泛化。 初学习器 调整分布，使得错误的样本有更高的关注（加权） 基于调制后的分布，循环训练下一个基训练器，直到T次循环结束。（生成新学习器） 结合策略平均法：简单平均、加权平均 投票法：绝对多数投票、相对多数投票、加权投票 学习法：通过元学习器结合 GBDTGBDT与传统的Boosting区别较大，它的每一次计算都是为了减少上一次的残差，在残差减小的梯度方向上建立模型。 所以说，在Gradient Boost中，每个新的模型的建立是为了使得之前的模型的残差往梯度下降的方法，与传统的Boosting中关注正确错误的样本加权有着很大的区别。GBDT的会累加所有树的结果，而这种累加是无法通过分类完成的，因此GBDT的树都是CART回归树，而不是分类树。 XGBoostXGBoost利用并行的CPU解决迭代次数过多问题 与GBDT区别 基学习器：GBDT以CART树作为基学习器，XGBoost还支持线性分类器，这个时候XGBoost相当于L1和L2正则化的逻辑斯蒂回归（分类）或者线性回归（回归）； 代价函数求导：传统的GBDT在优化的时候只用到一阶导数信息，XGBoost则对代价函数进行了二阶泰勒展开，得到一阶和二阶导数； 正则化：XGBoost在代价函数中加入了正则项，用于控制模型的复杂度。从权衡方差偏差来看，它降低了模型的方差，使学习出来的模型更加简单 缺失值的处理：XGBoost还可以自动学习出它的分裂方向 并行：多线程进行各个特征的增益计算（非Tree粒度的并行）","link":"/2020/04/20/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E5%86%B3%E7%AD%96%E6%A0%91/"},{"title":"java线程池ThreadPoolExecutor","text":"线程池优点重用性：，减少对象创建、消亡的开销，性能佳。并发控制：提高系统资源的使用率，同时避免过多资源竞争，避免堵塞。 ThreadPoolExecutor1234567public ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue, ThreadFactory threadFactory, RejectedExecutionHandler handler) {} corePoolSize线程池的核心线程数。在没有设置allowCoreThreadTimeOut为true的情况下，核心线程会在线程池中一直存活，即使处于闲置状态。 maximumPoolSize线程池允许创建的最大线程数。 keepAliveTime非核心线程闲置时的超时时长。超过该时长，非核心线程就会被回收。若allowCoreThreadTimeOut属性为true时，该时长同样会作用于核心线程。 unit：keepAliveTime的时间单位。 workQueue：线程池中的任务队列，通过线程池的execute()方法提交的Runnable对象会存储在该队列中。 可选子类： threadFactory： 线程工厂， RejectedExecutionHandler： 当队列和线程池满了，拒绝策略4种 1、抛异常(AbortPolicy) （默认） 2、丢弃队列中最老的任务(DiscardOldestPolicy)。 3、直接丢弃（DiscardPolicy） 4、将任务分给调用线程来执行(CallerRunsPolicy)。 处理流程 Executors提供的线程池 FixedThreadPool线程数量固定的线程池，无限的任务队列，只有核心线程。最多只有nThreads个任务在并行处理，之后都在排队等待。 12345public static ExecutorService newFixedThreadPool(int nThreads) { return new ThreadPoolExecutor(nThreads, nThreads, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue&lt;Runnable&gt;());} CachedThreadPool适合执行大量耗时较少的任务。没有核心线程，即没有任务时，它几乎不占用任何系统资源。SynchronousQueue不缓存任何一个任务，当即执行。 12345public static ExecutorService newCachedThreadPool() { return new ThreadPoolExecutor(0, Integer.MAX_VALUE, 60L, TimeUnit.SECONDS, new SynchronousQueue&lt;Runnable&gt;());} ScheduledThreadPool 创建一个定长线程池，支持定时及周期性任务执行。 12345678public static ScheduledExecutorService newScheduledThreadPool(int corePoolSize) { return new ScheduledThreadPoolExecutor(corePoolSize); } public ScheduledThreadPoolExecutor(int corePoolSize) { super(corePoolSize, Integer.MAX_VALUE, 10L, MILLISECONDS, new DelayedWorkQueue()); } SingleThreadExecutor 创建一个单线程化的线程池，它只会用唯一的工作线程来执行任务，保证所有任务按照指定顺序(FIFO, LIFO, 优先级)执行。 123456public static ExecutorService newSingleThreadExecutor() { return new FinalizableDelegatedExecutorService (new ThreadPoolExecutor(1, 1, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue&lt;Runnable&gt;()));}","link":"/2020/04/22/java%E7%BA%BF%E7%A8%8B%E6%B1%A0ThreadPoolExecutor/"}],"tags":[{"name":"Java","slug":"Java","link":"/tags/Java/"},{"name":"comm","slug":"comm","link":"/tags/comm/"},{"name":"python","slug":"python","link":"/tags/python/"},{"name":"算法","slug":"算法","link":"/tags/%E7%AE%97%E6%B3%95/"},{"name":"java","slug":"java","link":"/tags/java/"}],"categories":[]}